---
title: Readme
description: Vercel AI SDK: The AI Toolkit for TypeScript
**Version**: 1.0.0  
**Last Updated**: 2025-01-03  
**Purpose**: Building production-ready AI features with TypeScâ€¦
---

# Vercel AI Sdk: The AI Toolkit For Typescript
**Version**: 1.0.0  
## Table Of Contents

<!-- Generated placeholder; add anchors as needed -->

**Last Updated**: 2025-01-03  
**Purpose**: Building production-ready AI features with TypeScript

## ðŸŽ¯ Overview

The Vercel AI SDK is a TypeScript-first toolkit that simplifies building AI-powered features in web applications. It's provider-agnostic, optimized for streaming and edge runtimes, and designed to let developers focus on UX rather than low-level API calls.

### Core Philosophy
- **Start Small, Then Scale**: Get a minimal AI feature working quickly
- **Provider Agnostic**: Switch between OpenAI, Anthropic, etc. without rewriting
- **Streaming First**: Built for responsive, real-time experiences
- **Edge Ready**: Optimized for serverless and edge deployments
- **Type Safe**: Full TypeScript support throughout

## ðŸ—ï¸ Key Design Principles

### 1. Unified Interface
```typescript
import { openai } from '@ai-sdk/openai';
import { anthropic } from '@ai-sdk/anthropic';

// Same interface, different providers
const result = await generateText({
  model: openai('gpt-4'), // or anthropic('claude-3-opus')
  prompt: 'Write a haiku about TypeScript'
});
```

### 2. Streaming By Default
```typescript
// Server-side streaming
export async function POST(req: Request) {
  const { prompt } = await req.json();
  
  const result = await streamText({
    model: openai('gpt-4'),
    prompt,
  });
  
  return result.toAIStreamResponse();
}

// Client-side consumption
const { completion, isLoading } = useCompletion({
  api: '/api/completion'
});
```

### 3. React Hooks For AI
```typescript
function ChatComponent() {
  const { messages, input, handleSubmit, isLoading } = useChat({
    api: '/api/chat',
    onError: (error) => console.error(error)
  });
  
  return (
    <div>
      {messages.map(m => (
        <div key={m.id}>{m.role}: {m.content}</div>
      ))}
      <form onSubmit={handleSubmit}>
        <input value={input} onChange={(e) => setInput(e.target.value)} />
      </form>
    </div>
  );
}
```

### 4. Edge Runtime Optimization
```typescript
// Runs on Vercel Edge Runtime
export const runtime = 'edge';

export async function POST(req: Request) {
  const result = await generateText({
    model: openai('gpt-3.5-turbo'),
    prompt: 'Fast response from the edge',
    // Automatic token streaming for better UX
    streamProtocol: 'text',
  });
  
  return new Response(result.text);
}
```

## ðŸ’¡ Core Features

### 1. Multi-provider Support
```typescript
// Easy provider switching
const providers = {
  openai: openai('gpt-4'),
  anthropic: anthropic('claude-3-opus'),
  google: google('gemini-pro'),
  mistral: mistral('mistral-large'),
};

// Use based on requirements
const result = await generateText({
  model: providers[selectedProvider],
  prompt: userPrompt,
});
```

### 2. Structured Outputs
```typescript
import { z } from 'zod';

const schema = z.object({
  title: z.string(),
  summary: z.string(),
  tags: z.array(z.string()),
});

const result = await generateObject({
  model: openai('gpt-4'),
  schema,
  prompt: 'Analyze this article',
});

// Type-safe result
console.log(result.object.title); // TypeScript knows this exists
```

### 3. Tool Calling (Function Calling)
```typescript
const result = await generateText({
  model: openai('gpt-4'),
  prompt: 'What is the weather in San Francisco?',
  tools: {
    getWeather: {
      description: 'Get weather for a location',
      parameters: z.object({
        location: z.string(),
      }),
      execute: async ({ location }) => {
        const weather = await fetchWeather(location);
        return weather;
      },
    },
  },
});
```

### 4. Image Generation
```typescript
const result = await generateImage({
  model: openai.image('dall-e-3'),
  prompt: 'A minimalist logo for a TypeScript AI library',
  size: '1024x1024',
});

// Use the generated image
return new Response(result.base64, {
  headers: { 'Content-Type': 'image/png' },
});
```

## ðŸš€ Best Practices

### 1. Incremental Enhancement
```typescript
// Start simple
const { completion } = await generateText({
  model: openai('gpt-3.5-turbo'),
  prompt: 'Simple completion',
});

// Add streaming when needed
const stream = await streamText({
  model: openai('gpt-3.5-turbo'),
  prompt: 'Streaming response',
});

// Add advanced features as you scale
const advanced = await generateText({
  model: openai('gpt-4'),
  messages: conversationHistory,
  tools: availableTools,
  temperature: 0.7,
  maxTokens: 1000,
});
```

### 2. Error Handling
```typescript
try {
  const result = await generateText({
    model: openai('gpt-4'),
    prompt: userInput,
  });
} catch (error) {
  if (error instanceof AIError) {
    switch (error.code) {
      case 'rate_limit_exceeded':
        // Handle rate limiting
        await delay(error.retryAfter);
        break;
      case 'invalid_api_key':
        // Handle auth errors
        break;
      default:
        // Handle other errors
    }
  }
}
```

### 3. Cost Optimization
```typescript
// Use cheaper models for simple tasks
const classifier = await generateText({
  model: openai('gpt-3.5-turbo'), // Cheaper
  prompt: `Classify sentiment: "${text}"`,
  maxTokens: 10, // Limit output
});

// Use expensive models only when needed
if (requiresDeepAnalysis) {
  const analysis = await generateText({
    model: openai('gpt-4'), // More expensive
    prompt: complexAnalysisPrompt,
  });
}
```

### 4. Streaming Ux Patterns
```typescript
function StreamingResponse() {
  const { completion, isLoading, error } = useCompletion();
  
  return (
    <div>
      {/* Show skeleton while starting */}
      {isLoading && !completion && <Skeleton />}
      
      {/* Stream text as it arrives */}
      {completion && (
        <div className="prose">
          <ReactMarkdown>{completion}</ReactMarkdown>
        </div>
      )}
      
      {/* Show cursor while streaming */}
      {isLoading && completion && <Cursor />}
      
      {/* Handle errors gracefully */}
      {error && <ErrorMessage error={error} />}
    </div>
  );
}
```

## ðŸ“ Integration Patterns

### 1. API Routes Pattern
```typescript
// app/api/ai/route.ts
import { openai } from '@ai-sdk/openai';
import { streamText } from 'ai';

export async function POST(req: Request) {
  const { messages } = await req.json();
  
  const result = await streamText({
    model: openai('gpt-4'),
    messages,
  });
  
  return result.toAIStreamResponse();
}
```

### 2. Server Actions Pattern
```typescript
// app/actions.ts
'use server';

import { generateText } from 'ai';

export async function analyzeText(text: string) {
  const result = await generateText({
    model: openai('gpt-4'),
    prompt: `Analyze: ${text}`,
  });
  
  return result.text;
}
```

### 3. Edge Function Pattern
```typescript
// api/edge-ai.ts
export const config = { runtime: 'edge' };

export default async function handler(req: Request) {
  const result = await generateText({
    model: openai('gpt-3.5-turbo'),
    prompt: await req.text(),
  });
  
  return new Response(result.text);
}
```

## ðŸ”§ Claude Command Integration

### Development Commands
```bash
@~/Developer/docs/inspo/ai-and-modern-development/vercel-ai/
"Implement AI chat using Vercel AI SDK with:
- Streaming responses
- Error handling
- Provider switching
- Type safety"
```

### Architecture Commands
```bash
"Design a multi-provider AI system using Vercel AI SDK patterns:
- Abstract provider interface
- Cost-optimized model selection
- Fallback strategies
- Edge deployment ready"
```

### Optimization Commands
```bash
"Optimize this AI implementation for:
- Streaming performance
- Edge runtime compatibility
- Token usage efficiency
- User experience"
```

## ðŸŒŸ Key Advantages

### 1. **Future-Proof Architecture**
- Switch providers without rewriting code
- Adapt to new models as they release
- Provider competition benefits your app

### 2. **Production-Ready Features**
- Built-in retry logic
- Rate limit handling
- Streaming protocols
- Error boundaries

### 3. **Developer Experience**
- Full TypeScript support
- Intuitive React hooks
- Comprehensive documentation
- Active community

### 4. **Performance Optimized**
- Edge runtime support
- Streaming by default
- Minimal bundle size
- Efficient token usage

## ðŸ’­ Philosophy: "Start Small, Then Scale"

### Phase 1: Prototype
```typescript
// Get something working quickly
const response = await generateText({
  model: openai('gpt-3.5-turbo'),
  prompt: 'Hello AI!',
});
```

### Phase 2: Enhance
```typescript
// Add streaming for better UX
const stream = await streamText({
  model: openai('gpt-3.5-turbo'),
  prompt: userPrompt,
  onProgress: (chunk) => updateUI(chunk),
});
```

### Phase 3: Scale
```typescript
// Full production features
const system = {
  providers: [openai(), anthropic(), google()],
  cache: new RedisCache(),
  rateLimit: new RateLimiter(),
  monitoring: new DataDog(),
};

const result = await generateText({
  model: selectOptimalModel(system.providers, task),
  messages: conversationHistory,
  tools: availableTools,
  hooks: {
    onStart: () => system.monitoring.track('ai.request.start'),
    onComplete: (result) => system.cache.set(key, result),
  },
});
```

## ðŸ“š Resources

- [Vercel AI SDK Documentation](https://sdk.vercel.ai)
- [AI SDK Examples](https://github.com/vercel/ai/tree/main/examples)
- [Provider Comparison Guide](https://sdk.vercel.ai/providers)
- [Edge AI Patterns](https://vercel.com/docs/functions/edge-functions/ai)

---
*"Why Your First Step in AI Development Should Be Vercel's AI SDK" - Build incrementally, scale confidently*
---
title: Vercel AI: Modern AI Development
description: AI patterns and tooling from the Vercel ecosystem
---

## See Also

- [Standards](/architecture/standards)
- [Turborepo](/tools/stack/turborepo)
- [AI Sdk](/tools/stack/ai-sdk)
- [Orpc Turborepo Guide](/tools/integrations/orpc-turborepo-guide)
